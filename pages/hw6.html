<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 6</title>
    <link rel="stylesheet" href="../css/style.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>

<body>

    <aside id="sidebar-placeholder" class="sidebar"></aside>

    <main class="main-content">

        <header class="page-header">
            <h1>Homework 6</h1>
        </header>

    <main>
        
        <section class="card">
            
            <h2>Online Algorithms for Mean and Variance</h2>

            <h3>What is an "Online" Algorithm?</h3>
            <p>
                In statistics, a standard calculation (or "offline" algorithm) assumes 
                you have the <strong>entire dataset</strong> available in memory. To calculate the 
                mean, you sum all \(N\) values and divide by \(N\).
            </p>
            <p>
                An <strong>"online" algorithm</strong>, in contrast, processes data 
                sequentially, one piece at a time. It updates the statistic (like the mean) 
                using only the new value and the *previous* statistic. After processing 
                the new value, it is discarded.
            </p>
            <p>
                This approach is essential for:
            </p>
            <ul>
                <li><strong>Streaming Data:</strong> When data arrives continuously (e.g., IoT sensors, network traffic).</li>
                <li><strong>Large Datasets:</strong> When the dataset is too large to fit into memory.</li>
                <li><strong>Efficiency:</strong> It avoids re-calculating the entire sum every time a new value is added.</li>
            </ul>

            <hr style="margin: 25px 0;">

            <h3>1. Recurrence Formula for the Mean</h3>
            <p>
                Our goal is to find the new mean (\(\mu_n\)) using only the old mean 
                (\(\mu_{n-1}\)), the new value (\(x_n\)), and the new count (\(n\)).
            </p>
            
            <p><strong>Derivation:</strong></p>
            <ol>
                <li>
                    The mean at step \(n\) is defined as:
                    \[ \mu_n = \frac{1}{n} \sum_{i=1}^{n} x_i \]
                </li>
                <li>
                    We can split the sum into two parts: the sum of the first \(n-1\) items, and the new item \(x_n\):
                    \[ \mu_n = \frac{1}{n} \left( ( \sum_{i=1}^{n-1} x_i ) + x_n \right) \]
                </li>
                <li>
                    We know that the sum of the first \(n-1\) items is just \((n-1) \cdot \mu_{n-1}\). We can substitute this in:
                    \[ \mu_n = \frac{1}{n} \left( (n-1)\mu_{n-1} + x_n \right) \]
                </li>
                <li>
                    Now we just rearrange the terms to find the recurrence:
                    \[ \mu_n = \frac{(n-1)}{n}\mu_{n-1} + \frac{x_n}{n} \]
                    \[ \mu_n = \frac{n\mu_{n-1} - \mu_{n-1} + x_n}{n} \]
                    \[ \mu_n = \mu_{n-1} + \frac{x_n - \mu_{n-1}}{n} \]
                </li>
            </ol>
            
            <p>
                This final formula is the recurrence. It states:
                <br>
                <strong>New Mean = Old Mean + ( (New Value - Old Mean) / New Count )</strong>
            </p>

            <hr style="margin: 25px 0;">

            <h3>2. Recurrence Formula for the Variance</h3>
            <p>
                This is more complex. A naive online algorithm for variance is "numerically unstable" (can lead to errors). A robust method is <strong>Welford's Algorithm</strong>.
            </p>
            <p>
                Instead of tracking the variance directly, we track the "Sum of Squared Differences" from the mean, which we call \(M_n\).
            </p>
            <p>
                <strong>Definitions:</strong>
                <br>
                The sum of squares at step \(n\) is: \( M_n = \sum_{i=1}^{n} (x_i - \mu_n)^2 \)
                <br>
                The (population) variance is: \( \text{Var}_n = M_n / n \)
            </p>
            <p>
                The derivation for the recurrence of \(M_n\) is complex, but the result is surprisingly simple. It requires updating the mean first, and then updating \(M_n\).
            </p>
            
            <p><strong>The Recurrence Formulas (Welford's Algorithm):</strong></p>
            <p>
                To add a new value \(x_n\), we use these three steps (assuming we have \(n-1\), \(\mu_{n-1}\), and \(M_{n-1}\) stored):
            </p>
            <ol>
                <li>\( n = (n-1) + 1 \)</li>
                <li>\( \mu_n = \mu_{n-1} + \frac{x_n - \mu_{n-1}}{n} \)</li>
                <li>\( M_n = M_{n-1} + (x_n - \mu_{n-1})(x_n - \mu_n) \)</li>
            </ol>
            <p>
                After these calculations, the new variance can be found:
                <br>
                <strong>Population Variance:</strong> \( \sigma^2 = M_n / n \)
                <br>
                <strong>Sample Variance:</strong> \( s^2 = M_n / (n-1) \) (if \(n > 1\))
            </p>

            <hr style="margin: 25px 0;">
            
            <h3>3. Interactive Implementation (JavaScript)</h3>
            <p>
                This demo implements the "online" formulas derived above. Add values one 
                at a time, or add a batch of random values to see the statistics 
                update instantly.
            </p>
            
            <div class="interactive-demo">
                
                <div class="demo-group">
                    <label for="newValue">Enter a new value:</label>
                    <input type="number" id="newValue" placeholder="e.g., 5.3" style="width: 150px; margin-right: 10px;">
                    <button id="addValueBtn">Add Value</button>
                </div>

                <div class="demo-group" style="border-top: 1px solid #eee; padding-top: 15px; margin-top: 20px;">
                    <label for="randomCount">...or add a batch of random values (from 0 to 100):</label>
                    <input type="number" id="randomCount" value="100" style="width: 100px; margin-right: 10px;">
                    <button id="addRandomBtn">Add Random Values</button>
                </div>
                
                <div class="demo-group" style="margin-top: 20px;">
                     <button id="resetBtn" style="background-color:#dc3545;">Reset Statistics</button>
                </div>

                <div class="result-box" style="width: 100%; box-sizing: border-box; margin-top: 20px;">
                    <h4>Current Statistics:</h4>
                    <p style="font-size: 1.1em;">
                        <strong>Count (n):</strong> <span id="statCount">0</span>
                        <br>
                        <strong>Mean (\(\mu\)):</strong> <span id="statMean">N/A</span>
                        <br>
                        <strong>Sum of Squares (M<sub>n</sub>):</strong> <span id="statM2">N/A</span>
                        <br>
                        <strong>Population Variance (\(\sigma^2\)):</strong> <span id="statVar">N/A</span>
                        <br>
                        <strong>Sample Variance (s<sup>2</sup>):</strong> <span id="statSampleVar">N/A</span>
                    </p>
                </div>
            </div>

        </section>
        <section class="card">
            
            <h2>Optional: Computational and Numerical Advantages</h2>

            <p>
                The online algorithm (Welford's method) is superior to traditional batch 
                algorithms not only in efficiency but also, critically, in 
                <strong>numerical stability</strong>.
            </p>

            <h3>1. Computational Efficiency, Robustness, and Scalability</h3>
            <p>
                This is the most obvious advantage, as discussed previously:
            </p>
            <ul>
                <li>
                    <strong>Memory (Scalability):</strong> The online algorithm has a 
                    <strong>\(O(1)\) space complexity</strong> (it stores only 3 state 
                    variables: `count`, `mean`, `M2`). A batch algorithm requires 
                    <strong>\(O(N)\) space</strong> to store all data points, making 
                    it impossible for streams or terabyte-scale datasets.
                </li>
                <li>
                    <strong>Efficiency (Passes):</strong> The online algorithm requires only 
                    <strong>one pass</strong> over the data. The most stable batch algorithm 
                    requires <strong>two passes</strong> (one to find the mean \(\mu\), a second to find 
                    \( \sum (x_i - \mu)^2 \)).
                </li>
                <li>
                    <strong>Robustness:</strong> The algorithm is robust to data arrival; it can 
                    be stopped and started, and it produces a valid statistic at every step.
                </li>
            </ul>

            <hr style="margin: 25px 0;">

            <h3>2. Numerical Stability (The Critical Advantage)</h3>
            <p>
                The primary challenge in variance calculation is numerical precision. 
                A common "naive" one-pass batch formula is:
            </p>
            \[ \text{Var} = \frac{\sum x_i^2 - (\sum x_i)^2 / n}{n} \]
            
            <p>
                This formula is notoriously unstable and suffers from several problems 
                that Welford's algorithm (quello che abbiamo implementato) solves.
            </p>

            <h4>Catastrophic Cancellation</h4>
            <p>
                <strong>The Problem:</strong> This occurs when you subtract two very large 
                numbers that are very close to each other. The result loses many significant 
                digits of precision, leading to a highly inaccurate, small (or even negative) 
                variance.
            </p>
            <p>
                <em>Example:</em> Imagine your data is [1000000, 1000001, 1000002].
                <br>
                The term \( \sum x_i^2 \) will be massive.
                <br>
                The term \( (\sum x_i)^2 / n \) will also be massive, and almost identical to the first.
                <br>
                Their subtraction will result in catastrophic cancellation.
            </p>
            <p>
                <strong>The Online Solution:</strong> Welford's algorithm is built on calculating 
                the sum of squares using the *differences from the mean* (\( (x_n - \mu_{n-1})(x_n - \mu_n) \)). These differences (\(\delta\)) are 
                small numbers. By working only with these small deltas, it never 
                subtracts two massive numbers, thus <strong>completely avoiding catastrophic 
                cancellation</strong>.
            </p>

            <h4>Overflow Management</h4>
            <p>
                <strong>The Problem:</strong> In the naive formula, the intermediate term 
                \( \sum x_i^2 \) can grow astronomically large, potentially 
                exceeding the maximum value a standard 64-bit float can hold 
                (an <strong>overflow</strong>), even if the final variance itself is a small, 
                manageable number.
            </p>
            <p>
                <strong>The Online Solution:</strong> Welford's algorithm sums the squares of 
                *small differences*, not the squares of *large values*. The intermediate 
                sum \(M_n\) (our `M2` variable) grows much, much slower, making it 
                highly resistant to overflow.
            </p>

            <h4>Error Propagation</h4>
            <p>
                <strong>The Problem:</strong> In the naive algorithm, any small rounding error 
                (floating-point error) in the large sums is magnified during the final subtraction.
            </p>
            <p>
                <strong>The Online Solution:</strong> In Welford's, errors are not magnified 
                in the same way. The algorithm is "stable" because errors introduced 
                at one step tend to be corrected by future updates, rather than accumulating 
                and compounding. The running mean \(\mu_n\) "follows" the data, ensuring 
                the deltas stay small.
            </p>
        </section>
        

    </main>

    <script src="../js/hw6.js"></script>
    <script src="../js/nav-loader.js" defer></script>

</body>
</html>